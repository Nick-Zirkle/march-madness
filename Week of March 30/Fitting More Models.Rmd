---
title: "Fitting More Models"
author: "Nick Zirkle"
output: github_document
---

```{r setup, include=FALSE, warning=F, message=F}
library(ggplot2)
library(dplyr)
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
data <- read.csv("Andy's Dataset.csv")
```

#### Fitting a linear regression model based on Score Differential

```{r}
linear_fit_full <- lm(Score.Diff ~ Seed.Diff + Higher.Seed + COL.Diff + Higher.COL + DOL.Diff + Higher.DOL +
                   MOR.Diff + Higher.MOR + POM.Diff + Higher.POM +
                   SAG.Diff + Higher.SAG + WLK.Diff + Higher.WLK, data = data)
summary(linear_fit_full)
```

*As we can see from the summary output from above, only 3 of our predictor variables are "statistically significant" at p = 0.05: our intercept term, the higher seed variable, and the MOR ordinal ranking differential variable.*

```{r}
pred <- data.frame(Season = data$Season, Score.Diff = predict(object = linear_fit_full))

years <- seq(2003, 2022, by = 1)
years <- years[-c(18)]
averages <- rep(0, 19)
for (i in years) {
  temp_data <- data %>% filter(Season == i)
  temp_pred <- pred %>% filter(Season == i)
  num_games <- nrow(temp_data)
  if (i < 2020) {
    index <- i - 2002
  } else {
    index <- i - 2003
  }
  for (j in 1:num_games) {
    averages[index] <- averages[index] + (temp_data$Score.Diff[j] - temp_pred$Score.Diff[j])
  }
  averages[index] <- averages[index] / num_games
}

df <- data.frame(years = years, averages = averages)
ggplot(data = df, aes(x = years, y = averages)) + geom_bar(stat = "identity") +
  theme_bw() +
  xlab("Season") +
  ylab("Average Difference in Score") +
  ggtitle("Actual vs Predicted Score Differentials by Season")
```

*When we compare our fitted linear regression model to our observed score differentials, we notice that our model is "relatively" good at predicting score differentials, with 16 of the 19 observed seasons having an average difference between actual and predicted score differential within 2 points in either direction. One thing I noticed when running the predict() function was that our model was a lot better at predicting higher seed wins than lower seed wins, which makes sense considering the model is based on seeds and ordinal rankings.*

#### Fitting a logistic regression model based on Result

```{r}
for (i in 1:nrow(data)) {
  if (data$Result[i] == "Win") {
    data$Result[i] <- 1
  } else {
    data$Result[i] <- 0
  }
}
```

```{r}
data$Result <- as.numeric(data$Result)
log_fit_full <- glm(as.numeric(Result) ~ Seed.Diff + Higher.Seed + COL.Diff + Higher.COL + DOL.Diff + Higher.DOL +
                   MOR.Diff + Higher.MOR + POM.Diff + Higher.POM +
                   SAG.Diff + Higher.SAG + WLK.Diff + Higher.WLK, data = data, family = "binomial")
summary(log_fit_full)
```

*As we can see from the summary output from above, only 3 of our predictor variables are "statistically significant" at p = 0.05: our intercept term, the higher seed variable, and the higher COL ordinal ranking variable.*

```{r}
pred <- data.frame(Season = data$Season, Result = predict(object = log_fit_full, type = "response"))

years <- seq(2003, 2022, by = 1)
years <- years[-c(18)]
averages <- rep(0, 19)
for (i in years) {
  temp_data <- data %>% filter(Season == i)
  temp_pred <- pred %>% filter(Season == i)
  num_games <- nrow(temp_data)
  if (i < 2020) {
    index <- i - 2002
  } else {
    index <- i - 2003
  }
  for (j in 1:num_games) {
    averages[index] <- averages[index] + (temp_data$Result[j] - temp_pred$Result[j])
  }
  averages[index] <- averages[index] / num_games
}

df <- data.frame(years = years, averages = averages)
ggplot(data = df, aes(x = years, y = averages)) + geom_bar(stat = "identity") +
  theme_bw() +
  xlab("Season") +
  ylab("Average Difference in Result") +
  ggtitle("Actual vs Predicted Results by Season")
```

*When we compare our fitted logistic regression model to our observed results, we notice that our model is "relatively" good at predicting score differentials, with 18 of the 19 observed seasons having an average difference between actual and predicted result within 0.1 "prediction points" in either direction (prediction points can be seen as average percentage of error in this case). One thing I noticed when running the predict() function was that our model was a lot better at predicting higher seed wins than lower seed wins, which makes sense considering the model is based on seeds and ordinal rankings.*
